--- 
title: "Cellular Automata Control with Deep Reinforcement Learning"
author: "Emanuel Becerra Soto"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
classoption: openany
toc-depth: 3
lof: True
lot: True
bibliography: references.bib
biblio-style: apalike
link-citations: yes
github-repo: elbecerrasoto/diss
description: "Emanuel Becerra Soto Dissertation."
header-includes:
   - \usepackage{dsfont}
   - \usepackage{amsmath}
   - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
   - \usepackage{longtable}
   - \DeclareMathOperator*{\argmax}{arg\,max}
   - \usepackage[utf8]{inputenc}
---

```{r chunk-global, include=FALSE}
knitr::opts_chunk$set(echo=F, message=F, warning=F, dev="cairo_pdf")
```

# Abstract {-}

**The recent field of Deep Reinforcement Learning promises a new class of general algorithms capable of learning from mere real or simulated world interactions. Hence the field popularity has exploded in the last years. Despite its potential, real world concrete applications are lacking. This document proposes a novel Reinforcement Learning environment based on Cellular Automata, which are quintessential models of complex systems. Particularly a Forest Fire Cellular Automata was used as underlying dynamics for an agent that can affect the grid configuration by changing fire cells to empty cells. The semantics are those of a "Helicopter" trying to extinguishing a wild fire. We applied Deep Q-networks to solve the proposed environment on a $3 \times 3$ grid. We evaluate our agent on a 100,000 steps simulation of the environment. Our results show that Cellular Automata can be used as benchmark for Reinforcement Learning algorithms, likewise its modeling capabilities could help to narrow the gap to real world applications.**

\newpage

> "All of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal." [@sutton2018reinforcement]

>  "I prefer to eat a cake with a lot of cherries because I like reinforcement learning." Pieter Abbeel.

\newpage

# Acknowledgments {-}

Writing is hard. But this is a known fact. What is not known is the particular environment and situations that any writer must endure finishing its work. This thesis is dedicated to all the persons that somehow, consciously or unconsciously, have softened the weight and have let me only to struggle with the hard part of writing and not the harder part of "everything else".

To my family, including pets, for always being there, no matter what.

To my friends, who help me to put things in perspective and enjoy the ride.

To my professors, not only for their top-notch classes but giving me invaluable advice.

To my advisors, for transmitting me their passion for computer science and for
showing me that work-life balance is possible in this day and age but more importantly that one should love what it does.

To the _Centro de Investigación en Computación_ and the _Instituto Politécnico Nacional_ for providing a wonderful and life-changing experience.

This document was possible due to the economic support given by _Consejo Nacional de Ciencia y Tecnología_ (CONACYT).

\newpage
