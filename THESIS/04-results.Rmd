```{r}
suppressMessages(library(tidyverse))
suppressMessages(library(kableExtra))
```
# Experiments

## Materials and Methods

### Environment

A general tool for creating RL tasks was design (_Forest Fire Environment Maker (FFEM)_). The tool generates environments based on the forest fire CA, specifically the Drossel and Schwabl model (DSM) [@drossel1992self]. We developed the tool from an initial idea of an "helicopter" flying over the symbolic wild fire, represented by the DMS, trying to extinguish the fire by means of allowing the "helicopter" to change fire cells to empty cells. During the implementation the environment was naturally generalized. The basic idea is to have an agent on top of the CA lattice. The agent has its own state and parameters. Particularly it has an specified position at one of the nodes of the CA, where it can act by changing the cell on that position to another of its cell states. The agent navigates the CA changing cells states. When the agent reads its next instruction it moves to a new cell (or stays in place) and then affects, if applicable, the destination. After some agent movements the CA is updated and the full process is repeated.

The ability of allowing an agent to change the CA configurations express the attempt to drive the CA to a desired global state. In the RL framework this is done by giving a reward to the agent at each emitted action. This was done by mere counting all cell states indicating unwanted behavior and then multiplying by some weight (usually negative). This was generalized into getting the frequency of any cell type and then multiplying by a weight to generate a score per cell type. Then all the scores were combined into a global score. In summary the used reward function was the dot product of the cell-states frequencies by some weights, with the semantics that weights for desirable states are greater than the unwanted ones.

The particular environment that was used for this thesis is one that has the semantics of a "helicopter" trying to extinguish a forest fire. This is represented by an agent influencing the dynamics of a DSM and is as follows: The first component is a DSM with a lattice of size 3x3 with parameters $p=0.33$ (tree growth probability) and $f=0.066$ (tree spontaneous burning probability). The used boundary conditions were set to invariant using an extra exogenous cell type that it is not involved in any CA dynamics, we just called them "rocks". The starting lattice configuration was made via random sampling by cell with probability of choosing "tree" of $0.75$ and "fire" of $0.25$. The second component, the agent ("helicopter") has a starting potion in the middle of the lattice (_2nd row and column_), from there it can move in any direction of a Moore's neighborhood, thus the agent can choose from $9$ actions (_"Left-Up", "Up", "Right-Up", "Right", "Stay", "Left", "Left-Down", "Down", "Right-Down"_), then after arriving to its new destination the helicopter extinguishes the fire, represented by changing, if applicable, a "fire" cell to "empty". After some movements of the "helicopter" the forest fire CA is updated following the dynamics dictated by DSM. The specific sequence of agent movements and CA updates is: _move, update, move, move, update, move, move, update, move, move, ..._. In accordance to the RL paradigm, at each performed action the environment must respond with a reward signal, in our setting this was $+1$ per "tree" cell and $-1$ per "fire" cell, in other words a balance between trees and fires. Finally the RL task was continuous.

(ref:thesis-3-3-env) Illustration of two transitions in the proposed environment. On time $t$ the helicopter is at 2nd row and column, then it moves to the left, on arriving to its new position it changes the "fire" cell to "empty". Then, at the next time step, it returns to its original position, however the CA was updated before its arrival, anyway the "fire" cell at the middle (Not shown) is promptly eliminated and replaced by an "empty" cell. The parameter $m$ is an internal state of the environment, when it reaches $0$ the CA is updated at the next step, before the action takes place. The current reward $r$ is $+1$ per "tree" and $-1$ per "fire".
```{r thesis-3-3-env, fig.cap='(ref:thesis-3-3-env)', out.width='80%', fig.asp=.75, fig.align='center', echo=FALSE}
knitr::include_graphics('pics/agent-env_3x3.png')
```

### Code Availability

The source code for this thesis is openly available at:

1. https://github.com/elbecerrasoto/gym-forest-fire for the RL environments.
2. https://github.com/elbecerrasoto/CARL for the DQN implementations.

### Deep Q Networks

We tried to solve the proposed environment using model-free, value-function approximation RL. This approach is justified by the lack of general analytical models for CA dynamics [@ilachinski2001cellular] and the combinatorial explosion of states. For illustration in our small 3x3 environment the combinatorics are $3^9$ grid states multiplied by $3^2$ "helicopter" positions and $2$ internal states for the agent-environment synchronization, giving a grand total of $2 \times 3^2 \times 3^9 =$ `r 2 * 3^2 * 3^9`, a still manageable quantity by tabular RL standards, however merely scaling to a 16x16 grid leads to an untenable $\approx 10^{124}$ states.

Deep Q Networks (DQN) [@mnih2013playing][@mnih2015human] meets the previously stated conditions and is well suited for tasks with a discrete set of actions.

#### Preprocessing

#### Model Architecture

#### Training Details

### Evaluation Procedure

## Results

```{r 3x3, message=FALSE}
opts <- options(knitr.kable.NA = "")
read_csv('data/runs_3x3_v2.csv') %>%
  select(-name, -gamma, -better) %>%
  arrange(desc(return), sd) %>%
  mutate(mean = round(mean,2),
         sd = round(sd,2)) %>%
  knitr::kable(
    col.names = c('Run', 'Return', 'Mean', 'SD', 'Exploration', 'Unrolling', 'Architecture', 'LR', 'Batch Size'),
    booktabs=T,
    caption='Hyperparameters of 3x3 forest forest environments runs. The mean and standard deviation of reward per step were obtained from a 100,000 steps per model simulation.',
    format.args = list(big.mark = ",", scientific = F)
    ) %>%
  kable_styling(latex_options = c('striped', 'scale_down'))
```

## Disscusion
