[
["results.html", "5 Results ", " 5 Results "],
["materials-and-methods.html", "5.1 Materials and Methods", " 5.1 Materials and Methods Here is everything to replicate the experiments What exactly did I do? 5.1.1 Environment A general tool for creating RL tasks was design. The tool generates environments based on the forest fire CA, specifically the Drossel and Schwabl model (DSM) (Drossel and Schwabl 1992). The basic idea is to have an agent on top of the CA lattice. The agent has its own state and parameters. Particularly it has an specified postion at one of the nodes of the CA, where it can act by changing the cell on that postion to another of its cell states. The agent navigates the CA changing cells states. When the agent reads its next instruction it moves to a new cell (or stays in place) and then affects, if applicable, the destination. After some agent movements the CA is updated and the full process is repeated. The ability of allowing an agent to change the CA configurations express the attempt to drive the CA to a desired global state. In the RL framework this is done by giving a reward to the agent at each emited action. This was done by mere counting all cell states indicating unwanted behavior and multiplying by some weight (usually negative). This was generalized into getting the frecuency of any cell type and then multipliying by a weight to generate a score per cell type. Then all the scores were combined into a global score. In summary the used reward function was the dot product of the cell-states frecuencies by some weights, with the semantics that weights for desirable states are greater than the unwanted ones. The particular environment that was used for this thesis is as follows: A DSM on a lattice of 3x3 with the parameters \\(p=0.33\\) and \\(f=0.066\\) 5.1.2 Preprocessing 5.1.3 Code Availability The source code for this thesis is openly available at: https://github.com/elbecerrasoto/gym-forest-fire for the RL environments. https://github.com/elbecerrasoto/CARL for the DQN implementations. 5.1.4 Model Architecture 5.1.5 Training Details 5.1.6 Evaluation Procedure References "],
["results-and-disscusion.html", "5.2 Results and Disscusion", " 5.2 Results and Disscusion Profiling phase A life signals were detected using a 1x9 model. Then small charecterization of hte hyperameters was made using 3x3. Then scalation to 5x5. For a first approach to the proposed environment. Fisrt For the approaching of the problem a characterization of the … Table: (#tab:3x3) Hyperparamerters of 3x3 forest firest environments runs. Same f and p. Same suppressMessages(library(tidyverse)) read_csv(&#39;data/runs_3x3.csv&#39;) %&gt;% select(-name, -gamma) %&gt;% mutate(mean = round(mean,2), sd = round(sd,2)) %&gt;% knitr::kable( col.names = c(&#39;Run&#39;, &#39;Return&#39;, &#39;Mean&#39;, &#39;SD&#39;, &#39;Better than heuristic?&#39;, &#39;Exploration&#39;, &#39;Unrolling&#39;, &#39;Architecture&#39;, &#39;Batch Size&#39;, &#39;LR&#39;), longtable=T) ## Parsed with column specification: ## cols( ## name = col_character(), ## run = col_character(), ## return = col_double(), ## mean = col_double(), ## sd = col_double(), ## better = col_logical(), ## exploration = col_character(), ## unrolling = col_double(), ## architecture = col_character(), ## LR = col_double(), ## batch_size = col_double(), ## gamma = col_double() ## ) Run Return Mean SD Better than heuristic? Exploration Unrolling Architecture Batch Size LR c 640658 6.41 2.25 TRUE heuristic 2 A3 1e-04 16 a 638094 6.38 2.30 TRUE linear 2 A1 3e-04 32 d 615091 6.15 2.47 TRUE heuristic 3 A3 3e-04 32 i 591021 5.91 2.53 TRUE linear 2 A3 3e-04 32 g 507313 5.07 3.28 TRUE linear 10 A3 3e-04 32 b 327597 3.28 3.63 FALSE linear 1 A2 1e-04 256 h 294965 2.95 3.56 FALSE linear 1 A3 1e-04 16 f 293722 2.94 3.53 FALSE heuristic 1 A3 3e-04 32 e 293600 2.94 3.54 FALSE heuristic 1 A2 1e-04 16 read_csv(&#39;data/runs_3x3.csv&#39;) %&gt;% select(-name, -gamma) %&gt;% mutate(mean = round(mean,2), sd = round(sd,2)) ## Parsed with column specification: ## cols( ## name = col_character(), ## run = col_character(), ## return = col_double(), ## mean = col_double(), ## sd = col_double(), ## better = col_logical(), ## exploration = col_character(), ## unrolling = col_double(), ## architecture = col_character(), ## LR = col_double(), ## batch_size = col_double(), ## gamma = col_double() ## ) ## # A tibble: 9 x 10 ## run return mean sd better exploration unrolling architecture LR ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 c 640658 6.41 2.25 TRUE heuristic 2 A3 1.00e-4 ## 2 a 638094 6.38 2.3 TRUE linear 2 A1 3.00e-4 ## 3 d 615091 6.15 2.47 TRUE heuristic 3 A3 3.00e-4 ## 4 i 591021 5.91 2.53 TRUE linear 2 A3 3.00e-4 ## 5 g 507313 5.07 3.28 TRUE linear 10 A3 3.00e-4 ## 6 b 327597 3.28 3.63 FALSE linear 1 A2 1.00e-4 ## 7 h 294965 2.95 3.56 FALSE linear 1 A3 1.00e-4 ## 8 f 293722 2.94 3.53 FALSE heuristic 1 A3 3.00e-4 ## 9 e 293600 2.94 3.54 FALSE heuristic 1 A2 1.00e-4 ## # … with 1 more variable: batch_size &lt;dbl&gt; Numeber Scaling to 5x5 "],
["disscusion.html", "5.3 Disscusion", " 5.3 Disscusion The idea and contributions of the dissertation. Cellular Automata are discrete dynamical systems, that are well suited for the modelling of real world phenomens, particularly those who present characteristics of complex systems, such as emergent behaviour, chaotic behauvior, local behaviour, simple interactions but many actors. Nonetheless Cellular Automata field is rich with theory and any contribution to … In this work we have used the forest fire automata as an environment and he have defined a … We use state of the art deep reinforcement learning (DRL) algorithms to control the dynamics of cellular automata (CA) models. We assume there is an agent that can interact with the CA model by changing the state of one or more cells in the CA during each time step. The set of actions the agent can take vary with each specific CA model. For instance, in a CA that models the spread of fire, we model the agent as a helicopter flying over the CA that on each step can move to an adjacent cell and put out the fire of the cell in its current location. The goal of the agent is to minimize the spread of fire during the simulation. Our results show that DRL algorithms can control important aspects of CAs that model the behavior of real complex systems. On the other hand, this work shows that many CA models can be used as benchmarks for DRL algorithms that are closer to practical applications than many current benchmarks based on video games. "],
["results-and-disscusion-1.html", "5.4 Results and Disscusion", " 5.4 Results and Disscusion "],
["apendix-x-algorithms.html", "5.5 Apendix X: Algorithms", " 5.5 Apendix X: Algorithms 5.5.1 Algorithm 1 Just a sample algorithmn "]
]
