# Conclusion

## Apendix 1 Algorithms

\newcommand\mycommfont[1]{\small\ttfamily\textcolor{olive}{#1}}
\SetCommentSty{mycommfont}
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Steps to play $T$, Synchronization parameter $m$.}
\Output{Return $G$ (accumulated reward).}
\BlankLine
\SetKwFunction{computeForestFire}{computeForestFire}
\SetKwFunction{policy}{policy}
\SetKwFunction{updatePosition}{updatePosition}
\SetKwFunction{applyEffect}{applyEffect}
\SetKwFunction{getReward}{getReward}

Initialize: Automaton Grid $C_{i \times j}$

Initialize: Helicopter $H$: $H = (row, col): row \in \{1,...,i\}, col \in \{1,...,j\}$

$G \longleftarrow 0$

$k \longleftarrow m$

\For{1, ..., $T$}{
    \eIf{$k$ has value $0$}{
      $C_{i \times j} \longleftarrow \computeForestFire(C_{i \times j})$
      
      $k \longleftarrow m$
    }{
      $k \longleftarrow k - 1 $
    }
    $a \longleftarrow \policy(C_{i \times j},H,m)$
    
    $H \longleftarrow \updatePosition(H,a)$
    
    $C_{i \times j} \longleftarrow \applyEffect(C_{i \times j},H)$
    
    $r \longleftarrow \getReward(C_{i \times j})$
    
    $G \longleftarrow G + r$;
}
\caption{Agent-Environment interactions.}
\end{algorithm}

\newpage

\begin{algorithm}[H]

\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Steps to play $T$, \emph{Environment} and \emph{Agent} objects.}
\Output{Return $G$ (accumulated reward).}
\BlankLine
\SetKwFunction{Environment}{Environment}
\SetKwFunction{Agent}{Agent}

\tcc{Commonly used names: $s$ and $s'$ for current and new states, $a$ for action and $r$ for reward.}
\tcc{Environment object is reset to starting state, it returns the first observation}

$s \longleftarrow \Environment.reset()$

$G \longleftarrow 0$

$FALSE \longleftarrow terminationSignal$

\For{1, ..., $T$}{
    \eIf{$terminationSignal$}{
    
            \tcc{The episode is over, reset Environment again.}
            
            $s' \longleftarrow \Environment.reset()$
            
            $r \longleftarrow 0$
    }{
            $a \longleftarrow \Agent.policy(s)$
    
            $stepData \longleftarrow \Environment.step(a)$
    
            $s' \longleftarrow stepData[0]$
    
            $r \longleftarrow stepData[1]$
            
            $terminationSignal \longleftarrow stepData[2]$
    }
       
    $G \longleftarrow G + r$

    $s \longleftarrow s'$
}
\caption{Agent-Environment interactions following Open AI Gym API.}
\end{algorithm}

\newpage

\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Total iterations $T$ to train $Q$ network.}
\Output{Trained $Q$ network with weights $\theta$.}
\BlankLine
\SetKwFunction{Environment}{Environment}

Initialize replay memory $D$ to capacity $N$

Initialize policy network $Q$ with random weights $\theta$

Initialize target network $\hat{Q}$ with weights $\theta^-$ such that 
$\theta^- = \theta$

Get initial observation: $s \longleftarrow \Environment.reset()$

\For{t=1, ..., T}{

	With probability $\epsilon$: select random action $a$

	Otherwise select: $a \longleftarrow \underset {a'} {\argmax}\ Q(s,a';\theta)$
	
	Execute $\Environment.step(a)$ to obtain reward $r$, and next observation $s'$
	
	Store transition $(s,a,r,s')$ in $D$
	
	Sample a minibatch of transitions $(s_i,a_i,r_i,s'_i)$ from $D$
	
$
  y_i \longleftarrow \begin{cases}
               			r_i \text{\quad If the episode was over at iteration }i\\
               			r_i + \gamma \underset {a'} {\max}\ \hat{Q}\ (s'_i,a';\theta^-) \text{\quad Otherwise}
            		\end{cases}
$
	
	Perform optimization step on $\Big( y_i - Q(s_i,a_i;\theta) \Big)^2$ with respect to $\theta$ 
	
	Every $C$ steps synchronize networks weights: $\theta^- = \theta$
	
	\eIf{Episode was done at iteration t}{	
		$s \longleftarrow \Environment.reset()$
				
	}{
		
		$s \longleftarrow s' $
	}	
}
\caption{Deep Q-networks (DQN) using Open AI Gym API.}
\end{algorithm}


