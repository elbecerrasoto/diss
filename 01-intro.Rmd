# Introduction

## Motivations

Since antiquity the idea of building a thinking machine has always been in the minds of philosophers, artists, inventors, kings, and commoners alike, filling us with wonder, terror, and contemplation. A human creation capable of human feats would turn us, at least in an allegorical sense, into gods. This long time dream is an inspiration for the field of _Artificial Intelligence_ (AI). AI was born in 1956 on the _Dartmouth Summer Research Project on Artificial Intelligence_. In the early days of AI, the kind of problems that were tackled were the ones that are easy for a computer to solve but difficult for humans. These kinds of tasks are logical and mathematical in their very nature. Ironically the kind of tasks that are easy for humans have proved to be difficult to code on computers. Like visual identification of objects or navigation through complex environments. Because of their apparent easiness we seldom stop to think about how we actually manage to solve them and coming with a precise set of rules is labor-intensive and error-prone. An approach to solve those kinds of tasks is called _Machine Learning_ (ML). The basic idea of machine learning is to use data on the task at hand and let the computer figure out how to best use the data to solve the problem, in a figurative way learning from observation. The promise of such methods is avoiding explicit programming to solve problems and perhaps getting us closer to the faraway dream of a thinking machine. 

_Reinforcement Learning_ (RL) is a branch of ML that is inspired by how we learn through interaction with the real world. An agent interacts with an environment a receives a signal, called _reward_. The main idea of RL is that the agent should accumulate as much _reward_ as possible. In doing so it would accomplish whatever its purpose is. This is known as the _reward hypothesis_, in other words, getting plenty of favorable rewards is great for whatever the agent is trying to do. The agent, from its current knowledge, chooses an _action_ and then receives a _reward_ and an _observation_ from the environment, this process is repeated over and over while the agents seeks for its goal by accumulating _reward_. In summary, the _RL problem_ is how to correctly act to accomplish a goal by looking for behavior that maximizes an accumulated _reward_ quantity, know as _return_.

A popular perspective on AI and ML is deep learning. _Deep Learning_ is a myriad of techniques based on _artificial neural networks_ (ANN). ANN are loosely based in neuroscience and the brain structure. They are composed of several arrays of computing units called _neurons_. Neurons are arranged in hierarchies called layers. The processing of an ANN is done in an orderly fashion by the layers. When an input is received it is processed by the first layer, then the second and so on, until the last layer, which outputs the final result. If a technique or method is designated as _deep learning_ it only implies that such technique employs an ANN with a bunch of layers.

Naturally, RL and deep learning can be combined, this union is known as _Deep Reinforcement Learning_ (DRL). DRL has protagonized recent successes with superhuman performance on playing backgammon [@tesauro1995temporal], video games [@mnih2015human], poker [@moravvcik2017deepstack], multiplayer video games [@jaderberg2019human], chess [@silver2018general] and the decades-long challenging game of go [@silver2016mastering][@silver2018general].

One important aspect of advancing an AI area is to have adequate benchmarks. _Benchmarks_ are key problems that are widely used for the evaluation and design of algorithms. Having good performance on a benchmark is used to showcase the prowess of an algorithm. Nonetheless putting too much faith into a single benchmark could hurt research in the long run [@hooker1995testing][@stanley2015greatness]. Despite their pitfalls, they are essential tools of AI research. To name an important example, the _ImageNet_ dataset [@deng2009imagenet] and its associated challenge the _Large Scale Visual Recognition Challenge_ [@russakovsky2015imagenet] were fundamental for setting the current popularity of deep learning techniques and the bonanza in the field of computer vision.

In the scope of RL, a widely accepted benchmark is the _Arcade Learning Environment_ (ALE) [@bellemare2013arcade]. ALE is a suite of more than fifty games from the Atari 2600 console. Moreover, frameworks such as _Open AI Gym_ [@1606.01540] have lowered the barrier entry for using ALE, making Atari games easily accessible for RL algorithms and their training and evaluation. ALE's primary strengths are its variety of tasks and the independent creation of each game which reduces the bias of a single manufacturer. If a single algorithm is capable of performing well on a variety of very different and difficult tasks, it would be conceivable to claim its generality. Thus, ALE challenges any general RL algorithm to play well on as many as possible Atari games. A key event for the popularization of Atari games as a benchmark was their utilization on the seminal _Deep Q-Networks_ (DQN) papers [@mnih2013playing][@mnih2015human]. The DQN algorithm achieved a remarkable performance on a dozen of Atari games, receiving as input just raw pixels, hence without the games rules and from a single set of hyperparameters. DQN revived the interest in DRL methods.

Other common benchmarks for RL are, GridWorlds [@sutton2018reinforcement], robotics simulations with the MuJoCo physics engine [@todorov2012mujoco], classic control tasks [@1606.01540] and a heterogeneous variety of game theoretical tasks and board games [@lanctot2019openspiel].

Despite their broad success, Atari games suffer from their lack of environments with high stochasticity, which allows for the exploitation of such determinism by simply algorithms [@bellemare2015arcade][@machado2018revisiting]. Other criticisms arises from the emulation component of the games, that functionally transforms them into black boxes. This issue is compounded when some RL methods are also black boxes, making debugging extremely difficult [@foley2018toybox]. The emulation also hinders the parametrization of the environments and the extraction of meaningful statistics to broaden the understanding of why some RL algorithms work and others do not.

The central idea of this document is to show that _Cellular Automata_ (sg. _Cellular Automaton_) are suitable for the design and evaluation of RL environments. _Cellular Automata_ (CA) are mathematical and computational systems with interesting characteristics
and a rich history of theoretical background [@ilachinski2001cellular]. CA are systems with discrete and local dynamics and a discrete evolution. They have been traditionally used to model complex systems, on the natural and social sciences like the weather, wildfires, the immune system, pattern formation, galaxy formation, earthquakes, ecology, chemotaxis, epidemics, cell to cell interaction, reaction-diffusion systems, city traffic, social group formation, racial segregation, economics, just to name a few broad examples [@ganguly2003survey][@hoekstra2010simulating]. Furthermore, due to their complete discrete nature, CA can be seen as simple computational devices and provide an ideal model for parallel and unconventional computing. Of exceptional importance is that some CA have _universal computation_ capabilities, in other words, some CA have the same computing power of a _Turing Machine_ and thus capable of executing any algorithm.

_What makes a good benchmark?_ is a tough question to answer, since each field has its requirements and expectations, and tradeoffs must be made between many design dimensions such as _difficulty_, _relevance_, _accessibility_, _reproducibility_, _interpretability_ and so on. We believe that CA would make a good benchmark for RL for the following reasons:

1. Easiness of implementation.
2. Capable of modeling complex systems.
3. Computing models (_universal_, _parallel_, _unconventional_).
4. Highly parameterizable.

## Problem Statement

In this thesis, we consider the advantages of using Cellular Automata as underlying environments for RL tasks. To fully incorporate a Cellular Automaton into the RL framework a _Markov Decision Process_ (MDP) should be defined. So we need to be able to find appropriate sets of _Actions_ ($\mathcal{A}$), _States_ ($\mathcal{S}$) and _Rewards_ ($\mathcal{R}$), also specify an _Agent_ that interacts with the CA and explicitly or implicitly define the transitions of the MDP $p(s',r|s,a): \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$.

The particular CA that we would be using through this document is the _Drossel and Schwabl model_ (DSM) [@drossel1992self] also referred as the Forest Fire CA. DSM captures the dynamics of wildfires. Thus our first challenge is engineering a RL task that successfully integrates the DSM with some practical semantics. Thus, we propose an _Agent_ or "Helicopter" flying over the forest trying to extinguish the wildfire. For an initial characterization, a grid of $3 \ times 3$ will be used. Therefore, the set of all states $\mathcal{S}$ of the proposed environment will be the configurations of a $3 \times 3$ forest fire CA, with a tuple of helicopter positions $(row,col)$ and an internal parameter $m$ that takes two values $\{0, 1\}$ and controls the synchronization of helicopter movements and CA computations. The set of all actions $\mathcal{A}$ is the possible movement to all cells in a Moore's neighborhood from the agent position _{"Left-Up", "Up", "Right-Up", "Right", "Stay", "Left", "Left-Down", "Down", "Right-Down"_}. The rules that implicitly define the dynamics $p$ of the environment are:

1. The selection of any action would affect the "Helicopter" position tuple (unless the _"Stay"_ action was chosen), this is referred as "moving".
2. After moving to any cell the "Helicopter" extinguishes the fire by changing, if applicable, a current "fire" cell to an "empty" cell.
3. The $m$ parameter is decreased by $1$ after each "Helicopter" movement.
4. Before moving, if $m=0$ then the grid is updated following the Forest Fire CA rules, then $m$ is reset to its initial value ($m=1$).

After precisely defining the environment (see Appendix A \@ref(agent-environment-interactions)), the semantics are those of a helicopter trying to extinguish a wildfire in a simulated forest, thus the optimization problem that we want to solve is:

> Minimize the accumulated count of fire cells for a given time interval.

## Objectives

### Main Objectives

+ Propose a novel environment for Reinforcement Learning tasks, based on Cellular Automata, that could be used as an alternative benchmark instead of Atari games.
+ Characterize the environment by solving it by state of the art methods.

### Specific Objectives

+ Select a Cellular Automaton model for the environment, in this case the Forest Fire Cellular Automaton.
+ Design a RL task incorporating the CA dynamics.
+ Implementation of the RL environment, following the Open AI gym API.
+ Apply DQN to solve the proposed task.
